{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFJSBiasEvaluation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9B7PdsrvW__k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bias Evaluation for TF Javascript Model\n",
        "\n",
        "Based on the [FAT* Tutorial Measuring Unintended Bias in Text Classification Models with Real Data](https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/presentations/FAT_star_tutorial.md).\n",
        "\n",
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "metadata": {
        "id": "0Jsjp3E5rbuC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --quiet \"tensorflow>=1.11\"\n",
        "!pip3 install --quiet sentencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bSQf93oVo7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import sentencepiece\n",
        "from google.colab import auth\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# autoreload makes it easier to interactively work on code in imported libraries\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Set pandas display options so we can read more of the comment text.\n",
        "pd.set_option('max_colwidth', 300)\n",
        "\n",
        "# Seed for Pandas sampling, to get consistent sampling results\n",
        "RANDOM_STATE = 123456789"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12LU1AjWr-da",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4dzfYrvr3XT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download and unzip files used in this colab\n",
        "!curl -O -J -L https://storage.googleapis.com/civil_comments/fat_star_tutorial/fat-star.zip\n",
        "!unzip -o fat-star.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FFFXbLiRrvtz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p tfjs_model\n",
        "!gsutil -m cp -R gs://conversationai-public/public_models/tfjs/v1/* tfjs_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bmiyJR60gDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_df_float = pd.read_csv('public_test.csv')\n",
        "print('test data has %d rows' % len(test_df_float))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0SpGASQXm7O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to convert toxicity and identity columns to booleans, in order to work with our neural net and metrics calculcations.  For this tutorial, we will consider any value >= 0.5 as True (i.e. a comment should be considered toxic if 50% or more crowd raters labeled it as toxic).  Note that this code also converts missing identity fields to False."
      ]
    },
    {
      "metadata": {
        "id": "gf_Ra3fGVwK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# List all identities\n",
        "identity_columns = [\n",
        "    'male', 'female', 'transgender', 'other_gender', 'heterosexual',\n",
        "    'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian',\n",
        "    'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black',\n",
        "    'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
        "    'physical_disability', 'intellectual_or_learning_disability',\n",
        "    'psychiatric_or_mental_illness', 'other_disability']\n",
        "\n",
        "def convert_to_bool(df, col_name):\n",
        "  df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
        "\n",
        "def convert_dataframe_to_bool(df):\n",
        "  bool_df = df.copy()\n",
        "  for col in ['toxicity'] + identity_columns:\n",
        "      convert_to_bool(bool_df, col)\n",
        "  return bool_df\n",
        "\n",
        "test_df = convert_dataframe_to_bool(test_df_float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8RfGq2lX2EY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Score test set with our text classification model\n",
        "\n",
        "Using our new model, we can score the set of test comments for toxicity.\n"
      ]
    },
    {
      "metadata": {
        "id": "AfC_yo0Tt5SQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TOXICITY_COLUMN = 'toxicity'\n",
        "TEXT_COLUMN = 'comment_text'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0KT0565tUDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_fn = tf.contrib.predictor.from_saved_model(\n",
        "  'tfjs_model', signature_def_key='predict')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZppO68XctZPH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sp = sentencepiece.SentencePieceProcessor()\n",
        "sp.Load('tfjs_model/assets/universal_encoder_8k_spm.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3heBWS5tdg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSG_Dc7Gti-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tox_scores = []\n",
        "nrows = test_df.shape[0]\n",
        "out = display(progress(0, nrows), display_id=True)\n",
        "for offset in range(0, nrows):\n",
        "  out.update(progress(offset, nrows))\n",
        "  values = sp.EncodeAsIds(test_df[TEXT_COLUMN][offset])\n",
        "  tox_scores.append(predict_fn({\n",
        "      'values': values,\n",
        "      'indices': [(0, i) for i in range(len(values))],\n",
        "      'dense_shape': [1, len(values)]})['toxicity/probabilities'][0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0YjAtUBWa1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'fat_star_tutorial'\n",
        "test_df[MODEL_NAME] = tox_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P31u4dyyaKKE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate the overall ROC-AUC\n",
        "\n",
        "This calculates the models performance on the entire test set using the ROC-AUC metric."
      ]
    },
    {
      "metadata": {
        "id": "wMj48wLHX93N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_overall_auc(df, model_name):\n",
        "    true_labels = df[TOXICITY_COLUMN]\n",
        "    predicted_labels = df[model_name]\n",
        "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
        "\n",
        "calculate_overall_auc(test_df, MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6IClt8eplMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get a list of identity columns that have >= 100 True records.  This will remove groups such\n",
        "# as \"other_disability\" which do not have enough records to calculate meaningful metrics.\n",
        "identities_with_over_100_records = []\n",
        "for identity in identity_columns:\n",
        "    num_records = len(test_df.query(identity + '==True'))\n",
        "    if num_records >= 100:\n",
        "        identities_with_over_100_records.append(identity)\n",
        "\n",
        "SUBGROUP_AUC = 'subgroup_auc'\n",
        "BACKGROUND_POSITIVE_SUBGROUP_NEGATIVE_AUC = 'background_positive_subgroup_negative_auc'\n",
        "BACKGROUND_NEGATIVE_SUBGROUP_POSITIVE_AUC = 'background_negative_subgroup_positive_auc'\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "  try:\n",
        "    return metrics.roc_auc_score(y_true, y_pred)\n",
        "  except ValueError:\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
        "  subgroup_examples = df[df[subgroup]]\n",
        "  return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
        "\n",
        "\n",
        "def compute_background_positive_subgroup_negative_auc(df, subgroup, label, model_name):\n",
        "  \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
        "  subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
        "  non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
        "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
        "  return compute_auc(examples[label], examples[model_name])\n",
        "\n",
        "\n",
        "def compute_background_negative_subgroup_positive_auc(df, subgroup, label, model_name):\n",
        "  \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
        "  subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
        "  non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
        "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
        "  return compute_auc(examples[label], examples[model_name])\n",
        "\n",
        "\n",
        "def compute_bias_metrics_for_model(dataset,\n",
        "                                   subgroups,\n",
        "                                   model,\n",
        "                                   label_col,\n",
        "                                   include_asegs=False):\n",
        "  \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
        "  records = []\n",
        "  for subgroup in subgroups:\n",
        "    record = {\n",
        "        'subgroup': subgroup,\n",
        "        'subgroup_size': len(dataset[dataset[subgroup]])\n",
        "    }\n",
        "    record[SUBGROUP_AUC] = compute_subgroup_auc(\n",
        "        dataset, subgroup, label_col, model)\n",
        "    record[BACKGROUND_POSITIVE_SUBGROUP_NEGATIVE_AUC] = compute_background_positive_subgroup_negative_auc(\n",
        "        dataset, subgroup, label_col, model)\n",
        "    record[BACKGROUND_NEGATIVE_SUBGROUP_POSITIVE_AUC] = compute_background_negative_subgroup_positive_auc(\n",
        "        dataset, subgroup, label_col, model)\n",
        "    records.append(record)\n",
        "  return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
        "\n",
        "bias_metrics_df = compute_bias_metrics_for_model(test_df, identities_with_over_100_records, MODEL_NAME, TOXICITY_COLUMN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GS9t687KogDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot a heatmap of bias metrics"
      ]
    },
    {
      "metadata": {
        "id": "B5OxkxMqNvaB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot a heatmap of the bias metrics.  Higher scores indicate better results.\n",
        "* Subgroup AUC measures the ability to separate toxic and non-toxic comments for this identity.\n",
        "* Negative cross AUC measures the ability to separate non-toxic comments for this identity from toxic comments from the background distribution.\n",
        "* Positive cross AUC measures the ability to separate toxic comments for this identity from non-toxic comments from the background distribution."
      ]
    },
    {
      "metadata": {
        "id": "AGb1CQn2PZVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_auc_heatmap(bias_metrics_results, models):\n",
        "  metrics_list = [SUBGROUP_AUC, BACKGROUND_POSITIVE_SUBGROUP_NEGATIVE_AUC, BACKGROUND_NEGATIVE_SUBGROUP_POSITIVE_AUC]\n",
        "  df = bias_metrics_results.set_index('subgroup')\n",
        "  columns = []\n",
        "  vlines = [i * len(models) for i in range(len(metrics_list))]\n",
        "  for metric in metrics_list:\n",
        "    for model in models:\n",
        "      columns.append(metric)\n",
        "  num_rows = len(df)\n",
        "  num_columns = len(columns)\n",
        "  fig = plt.figure(figsize=(num_columns, 0.5 * num_rows))\n",
        "  ax = sns.heatmap(df[columns], annot=True, fmt='.2', cbar=True, cmap='Reds_r',\n",
        "                   vmin=0.5, vmax=1.0)\n",
        "  ax.xaxis.tick_top()\n",
        "  plt.xticks(rotation=90)\n",
        "  ax.vlines(vlines, *ax.get_ylim())\n",
        "  return ax\n",
        "\n",
        "plot_auc_heatmap(bias_metrics_df, [MODEL_NAME])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyv6n0GxpCwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot histograms showing comment scores"
      ]
    },
    {
      "metadata": {
        "id": "AKm925FWX_Fd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can graph a histogram of comment scores in each identity.  In the following graphs, the X axis represents the toxicity score given by our new model, and the Y axis represents the comment count.  Blue values are comment whose true label is non-toxic, while red values are those whose true label is toxic."
      ]
    },
    {
      "metadata": {
        "id": "iMSpM0U1YAN1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_histogram(non_toxic_scores, toxic_scores, description):\n",
        "  NUM_BINS=10\n",
        "  sns.distplot(non_toxic_scores, norm_hist=True, bins=NUM_BINS, color=\"skyblue\", label='non-toxic ' + description, kde=False)\n",
        "  ax = sns.distplot(toxic_scores, norm_hist=True, bins=NUM_BINS, color=\"red\", label='toxic ' + description, kde=False)\n",
        "  ax.set(xlabel='model toxicity score', ylabel='relative % of comments', yticklabels=[])\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "\n",
        "# Plot toxicity distributions of different identities to visualize bias.\n",
        "def plot_histogram_for_identity(df, identity):\n",
        "  toxic_scores = df.query(identity + ' == True & toxicity == True')[MODEL_NAME]\n",
        "  non_toxic_scores = df.query(identity + ' == True & toxicity == False')[MODEL_NAME]\n",
        "  plot_histogram(non_toxic_scores, toxic_scores, 'labeled for ' + identity)\n",
        "\n",
        "def plot_background_histogram(df):\n",
        "  toxic_scores = df.query('toxicity == True')[MODEL_NAME]\n",
        "  non_toxic_scores = df.query('toxicity == False')[MODEL_NAME]\n",
        "  plot_histogram(non_toxic_scores, toxic_scores, 'for all test data')\n",
        "\n",
        "# Plot the histogram for the background data, and for a few identities\n",
        "plot_background_histogram(test_df)\n",
        "plot_histogram_for_identity(test_df, 'heterosexual')\n",
        "plot_histogram_for_identity(test_df, 'transgender')\n",
        "plot_histogram_for_identity(test_df, 'homosexual_gay_or_lesbian')\n",
        "plot_histogram_for_identity(test_df, 'atheist')\n",
        "plot_histogram_for_identity(test_df, 'christian')\n",
        "plot_histogram_for_identity(test_df, 'asian')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}